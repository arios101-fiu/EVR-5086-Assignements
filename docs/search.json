[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EVR-5086 Assignments",
    "section": "",
    "text": "Introduction\nAlthough the EVR-5086 class is being taught using Python, my prior experience is with R. I am also fond of sharing my work on GitHub. I have learned how GitHub pages combined with Quarto and R Studio are an extraordinary resource for developing and maintaining lab notebooks. To get better at using these tools (and the reproducibility and accessibility of my future research) I have created a html quarto book and pdf to show my work associated with the course assignments.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#set-up",
    "href": "index.html#set-up",
    "title": "EVR-5086 Assignments",
    "section": "Set Up",
    "text": "Set Up\nI started by creating a GitHub account (username: arios101-fiu). Then, I created a GitHub repository with a gitignore and readme.md. Initially, the repository was called EVR-5086-Assignement1, but I updated it to (EVR-5086-Assignments). I cloned the repository into R Studio, thereby creating a R project. I copied in a _quarto.yml and index.qmd files from another project. I updated the files, rendered, committed, and pushed. Next, I turned on GitHub pages and updated the URLs in the yml and repository.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "assignment1/rios_evr5086_hw1.html",
    "href": "assignment1/rios_evr5086_hw1.html",
    "title": "1  Assignment 1 – Calculus Review",
    "section": "",
    "text": "Assignment 1 - Calculus Review",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Assignment 1 – Calculus Review</span>"
    ]
  },
  {
    "objectID": "assignment1/rios_evr5086_hw1.html#plot-the-polynomial",
    "href": "assignment1/rios_evr5086_hw1.html#plot-the-polynomial",
    "title": "1  Assignment 1 – Calculus Review",
    "section": "1.1 Plot the polynomial",
    "text": "1.1 Plot the polynomial\nBelow are the steps I took to complete the first part of EVR-5086 Assignment 1.\nIn doing this exercise in R, I started by loading the R libraries I will use in this chapter. I used {ggplot2} for plotting, and {tidyr} and {dplyr} for data wrangling.\n\n# Check if libraries are installed; install if not.\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(ggplot2, tidyr, dplyr)\n\nNext, I defined the variables and created the vectors I will need for the plot.\n\n# Define variables\na &lt;- 1\nn &lt;- 1\nb &lt;- 1\np &lt;- 2\nc &lt;- 1\nq &lt;- 3\n\n# Create x vector from -1 to 1\nx &lt;- seq(from = -1, to = 1, by = 0.1)\n\n# Calculate a value of y for each value of x\ny &lt;- (a * (x^n)) + (b * (x^p)) + (c * (x^q))\n\n# Calculate the analytical derivatives for each value of x\ndy_dx &lt;- (a * n * (x^(n - 1))) + (b * p * (x^(p - 1))) + (c * q * (x^(q - 1)))\n\n# Calculate the numerical derivatives between each value of x\ndeltay &lt;- diff(y)\ndeltax &lt;- diff(x)\ndeltay_deltax &lt;- deltay / deltax\n\n# For plotting purposes, derive the midpoint across the original values of x\ndeltax_vec &lt;- x[-length(x)] + deltax / 2\n\nMy next goal was to unite all of the vectors into a long data format. I did this by creating a data frame, then pivoting the data to only have the values that will be plotted on the x and y axis, as well as a label. Later, I will use my “linetype” label to define line types as well as the colors and shapes in my plot.\n\n# Build data frames and rename variables for plot\nplot_prep &lt;- data.frame(x, y, dy_dx) |&gt;\n  dplyr::rename(Polynomial = y, \"Analytical derivative\" = dy_dx)\n\n# Wrangle to long data format and bind in numerical derivative\nplot_tidy &lt;- plot_prep |&gt;\n  tidyr::pivot_longer(!x, names_to = \"linetype\", values_to = \"y\") |&gt;\n  dplyr::bind_rows(data.frame(x = deltax_vec, y = deltay_deltax,\n                              linetype = \"Numerical derivative\"))\n\nLastly, I create the plot and reflect on the observations and limitations of the numerical derivative.\nFigure 1.1 shows that the numerical derivative, shown as red open circles, is very similar to the analytical derivative, shown as a blue solid line. The good match we see relates to the scale over which we calculated the numerical derivative compared to the scale of the rate of change in the polynomial. When calculating the numerical derivative, we can get the average rate of change between two points.\nNote that for the analytical derivative we are only providing the plot with information associated with x values ranging -1 to 1, in steps of 0.1. Meanwhile, the numerical derivative is plotted at the midpoints of our original segments, with x values ranging from -0.95 to 0.95. Including the numerical derivatives in the appropriate position relative to the curved lines plotted between our analytical derivatives results in the overlay of the points and the line.\nIf the numerical derivative had a significantly lower resolution (e.g. just -1 and 1), it would not match well, and would be just one point, at x = 0, above the “U” shaped line representing the analytical derivative. Although such a wide spacing is extreme to consider, it helps to emphasize that grid spacing and location plotted are important considerations when working with numerical derivatives.\n\n# Plot the analytically derivative as a solid line\n# and the numerical derivative as open symbols\npolynomial_plot &lt;- ggplot(data = plot_tidy,\n                          aes(x = x, y = y, color = linetype)) +\n  geom_point(\n    data = dplyr::filter(plot_tidy, linetype == \"Numerical derivative\"),\n    shape = 21, stroke = 1.25\n  ) +\n  geom_line(\n    data = dplyr::filter(plot_tidy, linetype != \"Numerical derivative\")\n  ) +\n  theme(legend.title = element_blank()) +\n  scale_color_manual(values = c(4, 2, 1)) +\n  theme_minimal() +\n  theme(legend.title = element_blank()) +\n  labs(\n    title = \"Plot of polynomial with analytical and numerical derivatives\"\n  )\n\npolynomial_plot\n\n\n\n\n\n\n\nFigure 1.1: Polynomial defined by values provided in EVR-5086 Assignment 1 \\n (black line), along with analytical derivative (blue line) and numerical \\n derivative (red open circle).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Assignment 1 – Calculus Review</span>"
    ]
  },
  {
    "objectID": "assignment1/rios_evr5086_hw1.html#solve-the-2-d-laplace-in-excel",
    "href": "assignment1/rios_evr5086_hw1.html#solve-the-2-d-laplace-in-excel",
    "title": "1  Assignment 1 – Calculus Review",
    "section": "1.2 Solve the 2-D Laplace in Excel",
    "text": "1.2 Solve the 2-D Laplace in Excel\nI created a 28 by 28 grid of the 2-D Laplace Equation. I included three internal “boundary values”; one high value of 4 and two low values of -2 and -3. The two low values were near each other compared to their respective distances to the high value. I allowed excel to iteratively calculate for 10,000 iterations with a minimum change of 0.0001. I saved the file as a CSV file after including explicit zeros surrounding the formulas. The dimensions of my data were 30 by 30. I rounded to four significant digits to see if stagnation areas would be more evident by avoiding calculating of extremely small differences.\n\n1.2.1 Read in and plot contours using Python\nStart by turning on Python in R. This requires the package {reticulate} in R which embeds a Python session within the R session. The function py_require() is also used to declare Python packages that will be used in the R session.\n\n# Check if libraries are installed; install if not.\nif (!require(\"reticulate\")) install.packages(\"reticulate\")\n\nLoading required package: reticulate\n\n# Load reticulate\nlibrary(reticulate)\n\n# Ensures matplotlib package is available in the current session\nif (!py_module_available(\"matplotlib\")) py_require(c(\"matplotlib\"))\n\nThe rest of the assignment is run in Python. First, I import the numpy and matplotlib.pyplot packages and read in the CSV file that I had created in excel. To prepare the data for plotting, I create two arrays using np.linspace() and combine them into a 30 x 30 grid of x and y coordinates using np.meshgrid(). Finally, the partial derivatives for h with respect to x and y are calculated using np.gradient().\n\n# Import packages\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load csv file from excel\nh = np.loadtxt('tripole.csv',delimiter=',')\n\n# Create a grid of x and y coordinates\nx_vec = np.linspace(-1.5, 1.4, 30)\ny_vec = np.linspace(-1.5, 1.4, 30)\nX, Y = np.meshgrid(x_vec, y_vec)\n\n# Calculate gradient/partial derivatives\n[dhdy, dhdx] = np.gradient(h, y_vec, x_vec)\n\n# Round to 4 significant figures\ndhdy4 = np.round(dhdy, 4)\ndhdx4 = np.round(dhdx, 4)\n\nFigure 1.2 recreates the surface plot that perviously had been explored in Excel. The x- and y-axis range from -1.5 to 1.4, while the h-axis ranges from -3 to 4. Figure 1.3 shows a contour map with flow vectors. Finally, Figure 1.4 provides a similar plot to Figure 1.3, but with streamlines instead of arrows.\nReviewing the contours, flow vectors, and streamlines I did not identify stagnation points. When I selected the two low points, I was expecting a stagnation “saddle effect”. However, the proximity and the similarity in values I used did not result in a stagnation area. Interestingly, the majority of the surface plotted consisted of extensive areas of very low gradients. Figure 1.4 shows that the streams would run beyond the edges across approximately 60% of the plotted grid.\nThe following three Python code chunks created Figure 1.2, Figure 1.3 and Figure 1.4, respectively.\n\n\n1.2.2 Surface plot\n\nfig = plt.figure(figsize = [4, 4], dpi = 300) #Create empty figure\nax = plt.axes(projection = '3d') # Create plot region\nax.set_title(\" \" * 20 + 'Surface plot'+ \" \" * 20)  # Include plot title and pad space for h-axis label\nax.set_xlabel('x-axis') # Include x-axis label\nax.set_ylabel('y-axis')  # Include y-axis label\nax.set_zlabel('h-axis', rotation = 90) # Include vertical h-axis label\nsurf = ax.plot_surface(X,Y,h) # Plot surface\nplt.show() # Render plot\n\n\n\n\n\n\n\nFigure 1.2: Plot of vector arrows using Python. The vectors indicate strength and direction of the negative gradient. The vectors are displayed over the contours of a tri-pole solution with a high value of 4 and lows of -2 and -3. The two low values were near each other compared to their respective distances to the high value.\n\n\n\n\nplt.close('all') # Prevent accidental overplotting onto an old figure\n\n\n\n1.2.3 Plot contour map and flow vectors\n\n# Plot contour map and flow vectors\nplt.contourf(X, Y, h) # Draw contours for h on grid coordinates (x,Y)\ncbar = plt.colorbar() # Add colorbar\ncbar.set_label('Ground water potential surface (h)') # Include lable on colorbar\nplt.axis('equal'); # Force equal scaling on x and y\n\n(np.float64(-1.5), np.float64(1.4), np.float64(-1.5), np.float64(1.4))\n\nplt.title('Contour map and flow vectors') # Include plot title\nplt.xlabel('X-axis') # Include x-axis label\nplt.ylabel('Y-axis') # Include y-axis label\nqplt = plt.quiver(X, Y, -dhdx4, -dhdy4, scale = 360) # Draw vector arrows; note: large scales values make smaller arrows\nplt.show() # Render plot\n\n\n\n\n\n\n\nFigure 1.3: Plot of vector arrows using Python. The vectors indicate strength and direction of the negative gradient. The vectors are displayed over the contours of a tri-pole solution with a high value of 4 and lows of -2 and -3. The two low values were near each other compared to their respective distances to the high value.\n\n\n\n\nplt.close('all') # Prevent accidental overplotting onto an old figure",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Assignment 1 – Calculus Review</span>"
    ]
  },
  {
    "objectID": "assignment1/rios_evr5086_hw1.html#plot-streamlines-instead-of-arrows-in-sec-contour",
    "href": "assignment1/rios_evr5086_hw1.html#plot-streamlines-instead-of-arrows-in-sec-contour",
    "title": "1  Assignment 1 – Calculus Review",
    "section": "1.3 Plot streamlines instead of arrows in Section 1.2.3",
    "text": "1.3 Plot streamlines instead of arrows in Section 1.2.3\n\n# Plot contour map streamlines\nplt.contourf(X, Y, h) # Draw contours for h on grid coordinates (x,Y)\ncbar = plt.colorbar() # Add colorbar\ncbar.set_label('Ground water potential surface (h)') # Include lable on colorbar\nplt.axis('equal'); # Force equal scaling on x and y\n\n(np.float64(-1.5), np.float64(1.4), np.float64(-1.5), np.float64(1.4))\n\nplt.title('Contour map and streamlines') # Include plot title\nplt.xlabel('X-axis') # Include x-axis label\nplt.ylabel('Y-axis') # Include y-axis label\nplt.streamplot(X, Y, -dhdx4, -dhdy4) # Draw streamlines\n\n&lt;matplotlib.streamplot.StreamplotSet object at 0x000001ED0C238ED0&gt;\n\nplt.show() # Render plot\n\n\n\n\n\n\n\nFigure 1.4: Plot of streamlines using Python. The streamlines are displayed over the contours of a tri-pole solution with a high value of 4 and lows of -2 and -3. The two low values were near each other compared to their respective distances to the high value.\n\n\n\n\nplt.close('all') # Prevent accidental overplotting onto an old figure",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Assignment 1 – Calculus Review</span>"
    ]
  },
  {
    "objectID": "assignment1/rios_evr5086_hw1.html#links-to-colab-and-github",
    "href": "assignment1/rios_evr5086_hw1.html#links-to-colab-and-github",
    "title": "1  Assignment 1 – Calculus Review",
    "section": "1.4 Links to Colab and GitHub",
    "text": "1.4 Links to Colab and GitHub\nDraft code on Colab\nQuarto book chapter on GitHub",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Assignment 1 – Calculus Review</span>"
    ]
  },
  {
    "objectID": "assignment2/rios_evr5086_hw2.html",
    "href": "assignment2/rios_evr5086_hw2.html",
    "title": "2  Assignment 2 - Reading On-line Data and Visualizing Hurricane Tracks",
    "section": "",
    "text": "Assignment 2 - Reading On-line Data and Visualizing Hurricane Tracks",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Assignment 2 - Reading On-line Data and Visualizing Hurricane Tracks</span>"
    ]
  },
  {
    "objectID": "assignment2/rios_evr5086_hw2.html#sec-wrangle",
    "href": "assignment2/rios_evr5086_hw2.html#sec-wrangle",
    "title": "2  Assignment 2 - Reading On-line Data and Visualizing Hurricane Tracks",
    "section": "2.1 Data Retrieval and Parsing",
    "text": "2.1 Data Retrieval and Parsing\nThe purpose of this code is to access and format the HURDAT2 dataset into an analysis-ready format. Since I expect to explore these data further, I did not want to perform wrangling and formatting only on filtered subsets. Instead, I extracted the header information for each storm record, expanded it, and attached it to each corresponding data line.\nThere are many possible approaches to this task, including base R methods. I chose to use tidyr and dplyr because their piping syntax allows me to write modular code without overwriting objects or cluttering the global environment. While I could have created a single long script, I prefer a modular workflow where each section has a clear intention. For example, this part of the workflow focuses only on data retrieval and formatting.\nAt the end of Section 2.1, I save the resulting data frame as an .RDS file. I prefer RDS over RData because RDS requires explicit object naming when loaded, which supports better coding practices and clearer, more reproducible code.\nThe steps of the code are annotated in the code chunks, and the first chunk defines and loads all the required libraries used in Section 2.1. One limitation of my approach is that effective use or contribution requires familiarity with GitHub, RStudio, and Quarto.\n\n# Check if libraries are installed; install if not.\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(here, curl, tidyr, dplyr, lubridate)\n\n\n# Specify the source and output file name and location\nurl &lt;- \"https://www.nhc.noaa.gov/data/hurdat/hurdat2-1851-2024-040425.txt\"\ndestfile &lt;- here(\"assignment2\",\"hurdat.txt\")\n\n# Download and save the dataset\ncurl_download(url = url, destfile = destfile)\n\n\n# Read in data and differentiate between headers and data lines\nlines &lt;- readLines(\"hurdat.txt\")                     # Read text file\n\n# Prep headers\nstorm_headers &lt;- lines[grepl(\"^AL\", lines)]          # Keep only storm headers\nheader_parts &lt;- strsplit(storm_headers, \",\")         # Split based on \",\"\nheader_matrix &lt;- do.call(rbind, header_parts)        # Convert to matrix\nheader_df &lt;- as.data.frame(header_matrix)            # Convert to data frame\ncolnames(header_df) &lt;- c(\"storm_id\", \"name\", \"rows\") # Name columns\n\n# Repeat each header based on the 'rows' column in tidyr\nheader_expand &lt;- header_df |&gt;\n  mutate(rows = as.numeric(rows)) |&gt;\n  uncount(rows)\n\n# Prep data\nstorm_data &lt;- lines[!grepl(\"^AL\", lines)] # Keep only data\nhurdat_parts &lt;- strsplit(storm_data, \",\")   # Split based on \",\"\nhurdat_matrix &lt;- do.call(rbind, hurdat_parts) # Convert to matrix\nhurdat_df &lt;- as.data.frame(hurdat_matrix)     # Convert to to data frame\n\nhurdat_fields &lt;- c(\"yyyymmdd\", \"hhmm\", \"record\", \"status\", \n                 \"lat_hemi\", \"lon_hemi\", \"wind\", \"pressure\",\n                 \"ne34\", \"se34\", \"sw34\", \"nw34\",\n                 \"ne50\", \"se50\", \"sw50\", \"nw50\",\n                 \"ne64\", \"se64\", \"sw64\", \"nw64\", \"radius\")\n\ncolnames(hurdat_df) &lt;- hurdat_fields # Name columns\n\n# Build analysis ready data set\nhurdat_ar &lt;- header_expand |&gt;\n  bind_cols(hurdat_df) |&gt; # Glue together storm id and name with data\n  mutate(\n    yyyymmdd = ymd(yyyymmdd),                   # Tell R this is a date\n    hhmm = strptime(hhmm, format = \"%H%M\"),     # Tell R this is a time\n    hhmm = format(hhmm, \"%H:%M\"),\n    lat = as.numeric(substr(lat_hemi, 1, nchar(lat_hemi)-1)), # Remove \"S\"\n    lon = as.numeric(substr(lon_hemi, 1, nchar(lon_hemi)-1)), # Remove \"W\"\n    lat_hemi = substr(lat_hemi, nchar(lat_hemi), nchar(lat_hemi)),\n    lon_hemi = substr(lon_hemi, nchar(lon_hemi), nchar(lon_hemi)),\n    lat = if_else(lat_hemi == \"S\", -lat, lat), # Make lat negative if \"S\"\n    lon = if_else(lon_hemi == \"W\", -lon, lon)  # Make lon negative if \"W\"\n  ) |&gt;\n  mutate_at(c(9:25), as.numeric) |&gt;                  # Make data numeric\n  mutate(across(where(is.numeric), ~na_if(., -999))) # Replace NAs\n\n# Save formatted data to read into next quarto environment\nsaveRDS(hurdat_ar, file = here(\"assignment2\", \"hurdat.rds\"))",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Assignment 2 - Reading On-line Data and Visualizing Hurricane Tracks</span>"
    ]
  },
  {
    "objectID": "assignment2/rios_evr5086_hw2.html#sec-map",
    "href": "assignment2/rios_evr5086_hw2.html#sec-map",
    "title": "2  Assignment 2 - Reading On-line Data and Visualizing Hurricane Tracks",
    "section": "2.2 Data Visualization",
    "text": "2.2 Data Visualization\nThe data visualization has several exciting features. Because the data set was already formatted into an analysis-ready structure, this part of the assignment focuses only on visualizing a given storm ID. It begins with defining and loading the packages used later in the code. The leaflet and webshot2 packages were new to me. Leaflet allows me to create an interactive map, similar to folium in Python.\nSince I am rendering my report to both HTML and PDF, I needed different approaches for each format. In HTML, I was able to embed the leaflet map directly as an htmlwidget. For PDF output, I learned to use conditional content so the widget only displays in HTML, and a static snapshot (generated with webshot2) is included in the PDF. For both versions, I provided a figure caption and alt text to improve clarity and accessibility.\nTo add more complexity and depth to the track visualization, I incorporated a color scale representing wind speed. This makes the map more informative and highlights storm intensity changes along its path. I think interactive widgets can serve as a useful precursor to fully developed applications for data visualization. I am excited about the continued advancements in interactive figures which allow non-coders to explore and interact with data in more meaningful ways. Although I did not implement dynamic selection in the HTML rendering of this assignment, I looked into some of the latest developments in Quarto Dashboards. For now, I set up an optional user input similar to what we did in Python. When the R code is run interactively, the user is prompted to provide a storm name; otherwise, a default storm ID is used to ensure the code still runs smoothly during rendering.\n\n# Check if libraries are installed; install if not.\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(here, stringr, leaflet, webshot2, dplyr)\n\n\ndefault_name = \"AL092021\"\n\n# If interactive (R console / RStudio), ask the user\nif (interactive()) {\n  storm_id &lt;- readline(\n    prompt = paste0(\"Enter a storm ID using ALnnyyy format [default = \",   default_name, \"]: \")\n    )\n  if (storm_id == \"\") storm_id &lt;- default_name\n} else {\n  # If running non-interactively (e.g., knitting to PDF/HTML), use default\n  storm_id &lt;- default_name\n}\n\n\n# Read in data \nhurdat_ar &lt;- readRDS(here(\"assignment2\", \"hurdat.Rds\"))\n\n# Create a reusable function\ntrack_storm &lt;- function(dat, storm_id, zoom = 4,\n                            init_location = c(20, -50)) {\n  # Filter and order the points for the selected storm\n  storm &lt;- dat |&gt;\n    filter(storm_id == !!storm_id, !is.na(lat), !is.na(lon)) |&gt;\n    mutate(status = str_trim(status)) |&gt;\n    arrange(yyyymmdd, hhmm)\n\n  if (nrow(storm) == 0) stop(\"No points found for this storm_id.\")\n\n  # Build popup: date + time + status (e.g., \"1851-06-25 00:00 — HU\")\n  popup_txt &lt;- paste0(\n    format(storm$yyyymmdd, \"%Y-%m-%d\"), \" \", storm$hhmm,\n    \" — \", storm$status\n  )\n  \n  # Definecolor range\n  pal &lt;- colorNumeric(\n    palette = \"YlOrRd\",   # yelloe = weak winds, red = strong winds\n    domain = storm$wind   # The range of wind speeds\n  )\n  \n  # Create map\n  m &lt;- leaflet(storm) |&gt;\n  addTiles() |&gt;\n  addPolylines(lng = ~lon, lat = ~lat, color = \"blue\", \n               weight = 2.5, opacity = 1) |&gt;\n  addCircleMarkers(\n    lng = ~lon,\n    lat = ~lat,\n    color = ~pal(wind),       # marker color by wind\n    radius = 5,               # size of marker\n    stroke = FALSE,\n    fillOpacity = 0.8,\n    popup = ~paste0(format(yyyymmdd, \"%Y-%m-%d\"), \" \", hhmm,\n                    \"&lt;br&gt;Wind: \", wind, \" kt\",\n                    \"&lt;br&gt;Status: \", status)\n  ) |&gt;\n  addLegend(\n    \"bottomright\",\n    pal = pal,\n    values = ~wind,\n    title = \"Wind (kt)\",\n    opacity = 1\n  )\n  \n  file_html &lt;- here(\"assignment2\", paste0(storm_id, \"_map.html\"))\n  htmlwidgets::saveWidget(m, file_html, selfcontained = TRUE)\n  m\n}\n\nleaflet_png &lt;- function(m) {\n  file_png = here(\"assignment2\", \"hurricane_tracks_map.png\")\n  html_tmp &lt;- here(\"assignment2\", \"hurricane_tracks_map_tmp.html\")\n  htmlwidgets::saveWidget(m, html_tmp, selfcontained = TRUE)\n  webshot2::webshot(html_tmp, file = file_png, vwidth = 1400, \n                    vheight = 900, zoom = 1)\n  return(file_png)\n}\n\n\nm &lt;- track_storm(hurdat_ar, storm_id = storm_id)\nm\n\n\n\n\n\n\n\nFigure 2.1: Interactive storm track AL092021 (HTML only).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Assignment 2 - Reading On-line Data and Visualizing Hurricane Tracks</span>"
    ]
  },
  {
    "objectID": "assignment2/rios_evr5086_hw2.html#links-to-colab-and-github",
    "href": "assignment2/rios_evr5086_hw2.html#links-to-colab-and-github",
    "title": "2  Assignment 2 - Reading On-line Data and Visualizing Hurricane Tracks",
    "section": "2.3 Links to Colab and GitHub",
    "text": "2.3 Links to Colab and GitHub\nAssignment 2 Google Colab\nQuarto book chapter on GitHub",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Assignment 2 - Reading On-line Data and Visualizing Hurricane Tracks</span>"
    ]
  },
  {
    "objectID": "assignment3/rios_evr5086_hw3.html",
    "href": "assignment3/rios_evr5086_hw3.html",
    "title": "3  Assignment 3 - Normal distributions and the Galton board",
    "section": "",
    "text": "Assignment 3 - Normal distributions and the Galton board\nFor this assignment follows the readings and exercises in “Risk Analysis in the Earth Sciences: A Lab Manual with Exercises in R” Version 3.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Assignment 3 - Normal distributions and the Galton board</span>"
    ]
  },
  {
    "objectID": "assignment3/rios_evr5086_hw3.html#exercise-1",
    "href": "assignment3/rios_evr5086_hw3.html#exercise-1",
    "title": "3  Assignment 3 - Normal distributions and the Galton board",
    "section": "3.1 Exercise 1",
    "text": "3.1 Exercise 1\nIn this exercise I modified the provided lab0_sample.R to produce a histogram based on 104 samples from a normal distribution with mean 5 and standard deviation 2 Figure 3.1. I then produced two histograms for the standard normal distribution which has a mean 0 and standard deviation 1. The first was based on 104 samples (Figure 3.2) and the second one included only 10 samples (Figure 3.3). In addition to the modifications outlined by the exercise, I included a legend on each plot, and I dynamically read sample size into the y-axis label. I organized my code using variable names that differed by the subscript (1-3) associated with each of the three respective histograms. A main difference between Figure 3.1 and the other two histograms is how the distribution is centered around the mean of 5 and has a wider range of values compared to Figure 3.2. These differences are driven by the differences in the respective means and standard deviations. Since the first two histograms have a large number of samples being drawn from the normal distribution function rnorm(), they resulting plots show the expected bell curve shape. However, since the third histogram (Figure 3.3) is based on only 10 samples, we do not see the definition of the bell curve at all. When only using 10 samples, the standard normal distribution results in a smaller range of values than it did with 104 samples. When the number of samples is large, there are more opportunities for values to be sampled. I explore the related probability density in the next part of this exercise.\nIn the final part of this first exercise, I plotted the formula for the normal distribution provided in EVR-5086 Assignment 3 (Figure 3.4). I noticed a small difference between the equation in the assignment and the one on Wikipedia, which I included in the same plot as a second curve in blue. Overall, exploring these formulas helped me better understand how normal distribution’s characteristic symmetric bell curve shape is driven in its formula by e-x2. As shown in the examples in Figure 3.4 larger the value in the exponential term of the natural log, the wider the distribution. In this exercise I do not need to use the full formula that is on Wikipedia because, when mean is 0 and the standard deviation is 1, some constant multipliers simplify to 1. In the R code, I used text() and expression() to include color-coded mathematical expressions in the top-left corner of the plot.\n\n3.1.1 Modify code from lab0_sample.R\n\n# Modified code from lab0_sample.R\n\n# Set values\nnum_1 &lt;- 10^4   # number of random draws\nmu_1 &lt;- 5       # mean of normal distribution to draw from\nsigma_1 &lt;- 2    # standard deviation of normal distribution\n\n# Sample randomly from a normal distribution \nx_1 &lt;- rnorm(n = num_1, mean = mu_1, sd = sigma_1)\n\n# Plot the results as a histogram\nhist_1 &lt;- hist(\n  x_1,                 # Vector for the histogram\n  main = \"Adyan Rios\", # Set title to my name\n  xlab = \"Variable\",\n  ylab = paste0(\"Frequency (n = \", sprintf(\"%.2e\", num_1), \")\"),\n  xaxt = \"n\"           # Turn off x-axis values\n)\n\n# Add custom x-axis ticks\naxis(side = 1, at = hist_1$breaks)\n\n# Indicate mean\nabline(v = mu_1, lwd = 2, col = \"red\")\n\n# Indicate mean ± SD\nabline(v = c(mu_1 + sigma_1, mu_1 - sigma_1), \n       lwd = c(2), lty = 2, col = \"blue\")\n\n# Add a legend\nlegend(\"topleft\", legend = c(\"Mean\", \"Mean ± SD\"), lwd = 2, \n       col = c(\"red\", \"blue\"), lty = c(1, 2), bty = \"n\")\n\n\n\n\n\n\n\nFigure 3.1: Histogram generated from 1.00e+04 random samples from a normal distribution associated with mean 5 and standard deviation 2. The mean is indicated by a vertical red line. Blue dashed vertical lines show the mean minus the standard deviation and the mean plus the standard deviation.\n\n\n\n\n\n\n\n3.1.2 The Standard normal distribution\n\n# Set values\nnum_2 &lt;- 10^4   # number of random draws\nmu_2 &lt;- 0       # mean of normal distribution to draw from\nsigma_2 &lt;- 1    # standard deviation of normal distribution\n\n# Sample randomly from a normal distribution \nx_2 &lt;- rnorm(n = num_2, mean = mu_2, sd = sigma_2)\n\n# Plot the results as a histogram\nhist_2 &lt;- hist(\n  x_2,                 # Vector for the histogram\n  main = \"The Standard normal distribution\", # Set title \n  xlab = \"Variable\",   \n  ylab = paste0(\"Frequency (n = \", sprintf(\"%.2e\", num_2), \")\"),\n  xaxt = \"n\"           # Turn off x-axis values\n)\n\n# Add custom x-axis ticks\naxis(side = 1, at = hist_2$breaks)\n\n# Indicate mean\nabline(v = mu_2, lwd = 2, col = \"red\")\n\n# Indicate mean ± SD\nabline(v = c(mu_2 + sigma_2, mu_2 - sigma_2), \n       lwd = c(2), lty = 2, col = \"blue\")\n\n# Add a legend\nlegend(\"topleft\", legend = c(\"Mean\", \"Mean ± SD\"), lwd = 2, \n       col = c(\"red\", \"blue\"), lty = c(1, 2), bty = \"n\")\n\n\n\n\n\n\n\nFigure 3.2: Histogram generated from 1.00e+04 random samples from a normal distribution associated with mean 0 and standard deviation 1. The mean is indicated by a vertical red line. Blue dashed vertical lines show the mean minus the standard deviation and the mean plus the standard deviation.\n\n\n\n\n\n\n\n3.1.3 The Standard normal distribution with only 10 samples\n\n# Set values\nnum_3 &lt;- 10   # number of random draws\nmu_3 &lt;- 0       # mean of normal distribution to draw from\nsigma_3 &lt;- 1    # standard deviation of normal distribution\n\n# Sample randomly from a normal distribution \nx_3 &lt;- rnorm(n = num_3, mean = mu_3, sd = sigma_3)\n\n# Plot the results as a histogram\nhist_3 &lt;- hist(\n  x_3,                 # Vector for the histogram\n  main = \"The Standard normal distribution\", # Set title \n  xlab = \"Variable\",\n  ylab = paste0(\"Frequency (n = \", sprintf(\"%.2e\", num_3), \")\"),\n  xaxt = \"n\"           # Turn off x-axis values\n)\n\n# Add custom x-axis ticks\naxis(side = 1, at = hist_3$breaks)\n\n# Indicate mean\nabline(v = mu_3, lwd = 2, col = \"red\")\n\n# Indicate mean ± SD\nabline(v = c(mu_3 + sigma_3, mu_3 - sigma_3), \n       lwd = c(2), lty = 2, col = \"blue\")\n\n# Add a legend\nlegend(\"topleft\", legend = c(\"Mean\", \"Mean ± SD\"), lwd = 2, \n       col = c(\"red\", \"blue\"), lty = c(1, 2), bty = \"n\")\n\n\n\n\n\n\n\nFigure 3.3: Histogram generated from 1.00e+01 random samples from a normal distribution associated with mean 0 and standard deviation 1. The mean is indicated by a vertical red line. Blue dashed vertical lines show the mean minus the standard deviation and the mean plus the standard deviation.\n\n\n\n\n\n\n\n3.1.4 Plotting the normal distribution from Wikipedia\n\na &lt;- seq(-3, 3, length.out = 100)\n\nplot(a, 1 / (sqrt(2 * 3.14)) * exp(-(a^2)), col = \"red\", type = \"o\",\n     main = \"Probability density function\",\n     xlab = \"Variable x\", # Label x-axis\n     ylab = \"f(x)\")       # Label x-axis\npoints(a, 1 / (sqrt(2 * 3.14)) * exp( -(a^2) / 2), col = \"blue\")\ntext(-2, 0.35, expression(f(x) == 1 / (sqrt(2 %*% 3.14)) * exp( -(x^2) / 2)), \n     col = \"blue\", cex = 0.6)\ntext(-2, 0.3, expression(f(x) == 1 / (sqrt(2 %*% 3.14)) * exp( -(x^2))), \n     col = \"red\", cex = 0.6)\n\n\n\n\n\n\n\nFigure 3.4: Probability density plot for the normal distribution. The standard normal distribution (mean 0 and standard deviation 1) is shown by the curve in blue. Meanwhile, a normal distribution is also plotted in red with a slightly simplified equation.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Assignment 3 - Normal distributions and the Galton board</span>"
    ]
  },
  {
    "objectID": "assignment3/rios_evr5086_hw3.html#exercise-2",
    "href": "assignment3/rios_evr5086_hw3.html#exercise-2",
    "title": "3  Assignment 3 - Normal distributions and the Galton board",
    "section": "3.2 Exercise 2",
    "text": "3.2 Exercise 2\nIn this part of the assignment, I plotted sea-level data and temperature rates of change over time. To save time when re-running the code, I added if statements that only download the data if they do not exist locally.\nI ran into a couple early challenges. First, working with .txt files was a little challenging because I am used to .csv files, which usually already have descriptive column names and straightforward data frame structures. In trying a few options for how to read in the sea level data, I learned how to automatically ignore the comments beginning with “%” which streamline my code so I did not have to hard-code the lines to read. The next challenge was the way the time variable. I had not worked with decimal dates before, but I used date_decimal() from the lubridate to extract the dates into a format that was easier for me to interpret In reproducing and saving the three-panel plot (Atmospheric CO₂ (top), global mean surface-air temperature anomaly (middle), and global mean sea level anomaly (bottom)), I followed the code provided in lab1_sample.R. I then reproduced it in a multi-panel plot that renders directly in this document (Figure 3.5).\nTo answer how much atmospheric carbon dioxide concentrations, global mean temperatures, and sea levels changed between 1900 and the early part of the present century I subset each data set into “early” (1900-1910) and “recent” (2000-2010) and compared the mean values. The atmospheric carbon increased by 25.64 ppm (73%). Temperature and sea level both fluctuate between seasonally, but similar to the change seen for CO2, they also increased over those 100 years. The mean temperature increased by 0.93 degrees, and the mean sea level increased by 194 mm.\nThe last part of Exercise 2 involved calculating the rates of temperature change. I checked the data source to clarify why I was using column 14 and why the value was being divided by 100. I learned that the anomalies are stored as hundredths of a degree. Also, column 14 reflects information summarized across the 12 months, which are also provided in separate columns. Initially, I plotted the rate of change on the same axis as the original data series (Figure 3.6) using a red line for the rate and adding a legend. I also produced separate panels to more easily compare them (Figure 3.7). The results show that prior to 1970 there are fewer years with extreme increases or decreases. After the 1970, the dips and peaks are consistently more extreme in both direction. Although there are a few strong dips in the later half of the time series, summing over the values, indicated that the increases are more extreme overall. This corresponds with the upward trend observed in the global mean temperature anomaly plot, where the cyclical nature of the series is also evident.\n\n3.2.1 Sea level anomaly data\n\n# Check if libraries are installed; install if not.\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(here, lubridate, ggplot2)\n\n\n# Create a folder for storing downloaded files\nif (!file.exists(here(\"assignment3/data\"))) {\n  dir.create(here(\"assignment3/data\"))\n}\n\n# Download and read in the sea level anomaly data from Jevrejeva et al. (2014)\nif (!file.exists(here(\"assignment3/data/jevrejeva2014_gmsl.txt\"))) {\n  download.file(\n    \"https://psmsl.org/products/reconstructions/gslGPChange2014.txt\",\n    here(\"assignment3/data/jevrejeva2014_gmsl.txt\")\n  )\n}\n\n# Read in and ignore lines with comments (starting with %)\nsl.data &lt;-  read.table(here(\"assignment3/data/jevrejeva2014_gmsl.txt\"), \n                       comment = \"%\")\n\n# Assign column names\ncolnames(sl.data) &lt;-  \n  c(\"time\", \"rate_mm\", \"rate_err_mm\", \"gmsl_mm\", \"gmsl_err_mm\")\n\n# Format date\nsl.data$date_decimal &lt;- lubridate::date_decimal(sl.data$time)\n\n\n# Conditionally download files used in lab1_sample.R\nif (!file.exists(here(\"assignment3/data/co2_mm_mlo.txt\"))) {\n  download.file(\n    \"ftp://aftp.cmdl.noaa.gov/products/trends/co2/co2_mm_mlo.txt\",\n    here(\"assignment3/data/co2_mm_mlo.txt\")\n  )\n}\n\nif (!file.exists(here(\"assignment3/data/law2006.txt\"))) {\n  download.file(\n    \"ftp://ftp.ncdc.noaa.gov/pub/data/paleo/icecore/antarctica/law/law2006.txt\",\n    here(\"assignment3/data/law2006.txt\")\n  )\n}\n\nif (!file.exists(here(\"assignment3/data/GLB.Ts+dSST.txt\"))) {\n  download.file(\n    \"http://data.giss.nasa.gov/gistemp/tabledata_v3/GLB.Ts+dSST.txt\", \n    here(\"assignment3/data/GLB.Ts+dSST.txt\")\n  )\n}\n\n# Read in the CO2 data  \nloa.co2.data &lt;- read.table(here(\"assignment3/data/co2_mm_mlo.txt\"),\n                           skip = 57, header = FALSE)\nlaw.co2.data &lt;- read.table(here(\"assignment3/data/law2006.txt\"), \n                           skip = 183, nrows = 2004, \n                           header = FALSE)\n\n# Read in the GISS temperature data  \nbegin.rows &lt;- c(9, 31, 53, 75, 97, 119, 141)\nnum.rows &lt;- c(19, 20, 20, 20, 20, 20, 14)\ntemp.data &lt;- matrix(NA, nrow = sum(num.rows), ncol = 20)\ntemp.data[1: num.rows[1], ] &lt;- as.matrix(\n  read.table(\"data/GLB.Ts+dSST.txt\", skip = begin.rows[1], \n             nrows = num.rows[1], header = FALSE)\n)\nfor (i in 2: length(begin.rows)) {\n  temp.data[(sum(num.rows[1: i- 1])+ 1): sum(num.rows[1: i]), ] &lt;- \n    as.matrix(read.table(\"data/GLB.Ts+dSST.txt\", skip = begin.rows[i], \n                         nrows = num.rows[i], header = FALSE))\n}\n\n\n# Create a folder to store figues as pdfs  \nif (!file.exists(here(\"assignment3/figures\"))) {\n  dir.create(here(\"assignment3/figures\"))\n}\n\n# Plot\npdf(here(\"assignment3/figures/lab1_sample_plot2.pdf\"),\n    width = 4.5, height = 6)\npar(mfrow = c(3, 1), cex = 0.66)\nplot(law.co2.data[, 1], law.co2.data[, 6], type = \"l\", xlim = c(1900, 2020), \n     ylim = c(290, 400), bty = \"n\", xlab = \"Time (yr)\", \n     ylab = \"Atmospheric carbon dioxide (ppm)\")\nlines(loa.co2.data[, 3], loa.co2.data[, 5], type = \"l\", col = \"blue\")\nlegend(\"topleft\", c(\"Law Dome ice core record\", \"Mauna Loa measurements\"), \n       col = c(\"black\", \"blue\"), lwd = 1, bty = \"n\")\nplot(temp.data[, 1], temp.data[, 14]/ 100, type = \"l\", xlim = c(1900, 2020), \n     ylim = c(-0.6, 0.7), bty = \"n\", xlab = \"Time (yr)\", \n     ylab = \"Global mean temperature anomaly (K)\")\nplot(sl.data$time, sl.data$gmsl_mm , type = \"l\", xlim = c(1900, 2020),\n      ylim = c(-50, 200), bty = \"l\", xlab = \"Time (yr)\", \n     ylab = \"Sea level anomoly (mm)\")\n\n# Close the device and make the return value invisible\ninvisible(dev.off())\n\n# Re run code to print in Quarto html and pdf\n\n# CO2\nplot(law.co2.data[, 1], law.co2.data[, 6],\n     type = \"l\", xlim = c(1900, 2020), ylim = c(290, 400),\n     bty = \"n\", xlab = \"Time (yr)\", ylab = \"Atmospheric CO2 (ppm)\")\nlines(loa.co2.data[, 3], loa.co2.data[, 5], type = \"l\", col = \"blue\")\nlegend(\"topleft\",\n       c(\"Law Dome ice core record\", \"Mauna Loa measurements\"),\n       col = c(\"black\", \"blue\"), lwd = 1, bty = \"n\")\n\n# Temperature anomaly\nplot(temp.data[, 1], temp.data[, 14] / 100,\n     type = \"l\", xlim = c(1900, 2020), ylim = c(-0.6, 0.7),\n     bty = \"n\", xlab = \"Time (yr)\",\n     ylab = \"Global mean temperature anomaly (K)\")\n\n# Sea level anomaly\nplot(sl.data$time, sl.data$gmsl_mm,\n     type = \"l\", xlim = c(1900, 2020), ylim = c(-50, 200),\n     bty = \"n\", xlab = \"Time (yr)\",\n     ylab = \"Sea level anomaly (mm)\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Atmospheric CO2\n\n\n\n\n\n\n\n\n\n\n\n(b) Global mean temperature anomaly\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Sea level anomaly (mm)\n\n\n\n\n\n\n\nFigure 3.5: Atmospheric CO₂ (top), global mean surface-air temperature anomaly (middle), and global mean sea level anomaly (bottom), 1900–~2015.\n\n\n\n\n\n3.2.2 lab1_sample.R Question 1\nBy how much have atmospheric carbon dioxide concentrations, global mean temperatures, and sea levels changed between 1900 and the early part of the present century?\n\n# Create filtered subsets of each data set to calculate differences\nearly_co2 &lt;- law.co2.data[law.co2.data$V1 &gt;= 1900 & law.co2.data$V1 &lt; 1910, ]\nrecent_co2 &lt;- law.co2.data[law.co2.data$V1 &gt;= 2000 & law.co2.data$V1 &lt; 2010, ]\nearly_temp &lt;- temp.data[temp.data[, 1] &gt;= 1900 & temp.data[, 1] &lt; 1910, ]\nrecent_temp &lt;-temp.data[temp.data[, 1] &gt;= 2000 & temp.data[, 1] &lt; 2010, ]\nearly_sea &lt;- sl.data[floor(sl.data$time) &gt;= 1900 & floor(sl.data$time) &lt; 1910, ]\nrecent_sea &lt;- sl.data[floor(sl.data$time) &gt;= 2000 & floor(sl.data$time) &lt; 2010, ]\n\n# Calculate percent changes (c) or percent differences (pc)\npc_co2 &lt;- round((mean(recent_co2[, 6]) - mean(early_co2[, 6])) / \n                  mean(early_co2[, 6]), 4)*100 #25.64 percent\nc_co2 &lt;- mean(recent_co2[, 6]) - mean(early_co2[, 6]) #73.2 percent\nc_temp &lt;- mean(recent_temp[, 14]/100) - mean(early_temp[, 14]/100) #0.93 degrees\nc_sea &lt;- mean(recent_sea$gmsl_mm) - mean(early_sea$gmsl_mm) #194 mm\n\n\n\n3.2.3 Rates of temperature change\n\n# Rate of change\ndT_dt_1 &lt;- diff(temp.data[, 14])/100 / diff(temp.data[, 1])\nmidpoint_t &lt;- temp.data[-length(temp.data[, 1]), 1] + .5\n\n# GISS records sometimes use 100 to represent the average temperature\npar(mar = c(5, 7, 4, 2) + 0.1)\nplot(temp.data[, 1], temp.data[, 14] / 100, type = \"l\", xlim = c(1900, 2020), \n     ylim = c(-0.6, 0.9), bty = \"n\", xlab = \"Time (yr)\", \n     ylab = \"Global mean temperature anomaly (K)\n     and annual rate of change\", lwd = 1.5)\nlines(midpoint_t, dT_dt_1, type = \"l\", col = \"red\", lwd = 1.5)\n# Add a legend\nlegend(\"topleft\", \n       legend = c(\"Global mean temperature anomaly (K)\", \n                  \"Annual rate of temperature change\"), \n       lwd = 1.5, col = c(\"black\", \"red\"), lty = 1, bty = \"n\", cex = 0.8)\n\n# Second y-axis for the rate\n# plot(temp.data[, 1], temp.data[, 14] / 100, \n#      type = \"l\", xlim = c(1900, 2020),\n#      ylim = c(-0.6, 0.7), bty = \"n\", xlab = \"Time (yr)\",\n#      ylab = \"Global mean temperature anomaly (K)\", lwd = 1.5)\n# par(new = TRUE) # Prepare for second axis\n# plot(midpoint_t, dT_dt_1, type = \"l\", \n#      xlim = c(1900, 2020), ylim = c(-0.5, 0.5),\n#      col = \"red\", xaxt = \"n\", yaxt = \"n\", xlab = \"\", ylab = \"\", , lwd = 1.5)\n# axis(side = 4, col = \"red\", col.axis = \"red\")\n# mtext(\"Annual rate of change\", side = 4, line = 3, col = \"red\")\n\n\n\n\n\n\n\nFigure 3.6: Overalaid plots of global mean temperature anomaly (black) and its annual rate of change (red).\n\n\n\n\n\n\nplot(temp.data[, 1], temp.data[, 14] / 100, type = \"l\", xlim = c(1900, 2020), \n     ylim = c(-0.6, 0.7), bty = \"n\", xlab = \"Time (yr)\", \n     ylab = \"Global mean temperature anomaly (K)\", lwd = 1.5)\n\nplot(midpoint_t, dT_dt_1, type = \"l\", col = \"red\", lwd = 1.5, \n     xlim = c(1900, 2020),, ylim = c(-0.3, 0.3), bty = \"n\", \n     xlab = \"Time (yr)\", ylab = \"Annual rate of change\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Global mean temperature anomaly\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Annual rate of change\n\n\n\n\n\n\n\nFigure 3.7: Global mean temperature anomaly (black) and its annual rate of change (red).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Assignment 3 - Normal distributions and the Galton board</span>"
    ]
  },
  {
    "objectID": "assignment3/rios_evr5086_hw3.html#exercise-3",
    "href": "assignment3/rios_evr5086_hw3.html#exercise-3",
    "title": "3  Assignment 3 - Normal distributions and the Galton board",
    "section": "3.3 Exercise 3",
    "text": "3.3 Exercise 3\nTo prepare for this exercise, I included example code that was provided in the lab manual. I left it in because I reuse parts of it later, including the sampling examples to identify resulting “Galton Board” bins, and the code to generate histograms and Q-Q plots. For the actual exercise further down in this chapter, I created a function I could reuse for each variation across different number of balls. Using the function with intentional return vectors allowed me not to worry about clearing existing variables or figures from memory. I ran the function with 10 bins, and with 10, 20, 30, 100, and 10^4 balls. With 10 and 20 balls the histogram and Q-Q plot do not look normal. At about 30 balls the histogram and Q-Q plot begin to look approximately normal (Figure 4.3). Once the number of balls is large, as in the runs with 102 (Figure 4.4) and very large with 104 balls (Figure 4.5), the histogram looks normal but the Q-Q plot shows a slight departure. This may relate to the discrete and bounded nature of the bin outcomes, while the Q-Q plot may be better suited for continuous data.\n\n3.3.1 Implement the Galton Board\n\n# Check if libraries are installed; install if not.\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(animation)\n\n\n# Set number of balls and rows following the example code\nn.balls &lt;- 200\nn.rows &lt;- 15\n\n# ani.options(nmax = n.balls + n.rows - 2)\n# quincunx(balls = n.balls, layers = n.rows)\n\n\n# Follow example code to identify the resulting bin \npath &lt;- sample(x = c(-0.5, 0.5), size = (n.rows - 1), replace = TRUE)\nprint(path)\n\n [1] -0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5 -0.5  0.5 -0.5  0.5\n\nbin &lt;- sum(path)\nprint(bin)\n\n[1] 4\n\n\n\n# Example of a for loop\nn.times &lt;- 3\nfor (i in 1:n.times) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n\n\n\n# Another example of a for loop\nn.times &lt;- 5\noutput &lt;- rep(1, n.times)\nfor(i in 3:n.times){\n  output[i] &lt;- sum(output[(i-2):(i-1)])\n}\nprint(output)\n\n[1] 1 1 2 3 5\n\n\n\n# Example of how to make the Q-Q plot\nnorm.vals &lt;- rnorm(100, mean = 5, sd = 3)\nqqnorm(norm.vals)\nqqline(norm.vals)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Assignment 3 - Normal distributions and the Galton board</span>"
    ]
  },
  {
    "objectID": "assignment3/rios_evr5086_hw3.html#exercise-4",
    "href": "assignment3/rios_evr5086_hw3.html#exercise-4",
    "title": "3  Assignment 3 - Normal distributions and the Galton board",
    "section": "4.1 Exercise 4",
    "text": "4.1 Exercise 4\nBased on the histograms and Q–Q plots, the waiting time between Old Faithful eruptions is not approximately normally distributed (Figure 4.5). The histogram is clearly bimodal, and the Q–Q plot shows systematic departures from the red reference line, indicating a lack of correspondence between the theoretical normal quantiles and the sample quantiles. By contrast, based on the shape of the histogram and the close alignment with the red line in the Q–Q plot in Figure 4.7, the sepal length data for Iris setosa appears to be approximately normally distributed.\n\n4.1.1 Generate histograms and Q-Q plots\n\n4.1.1.1 Old Faithful eruptions\n# Histogram of waiting times\nhist(faithful$waiting,\n     main = \"Histogram of Waiting Times\",\n     ylab = paste0(\"Frequency (n = \", length(faithful$waiting), \")\"),\n     xlab = \"Waiting time between eruption (minutes)\")\n\n# Q-Q plot of waiting times\nqqnorm(faithful$waiting,\n       main = \"Q-Q Plot of Waiting Times\")\nqqline(faithful$waiting, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\n(a) Histogram\n\n\n\n\n\n\n\n\n\n\n\n(b) Q-Q Plot\n\n\n\n\n\n\n\nFigure 4.6: Histogram and Q-Q plot for the waiting time between eruptions of the Old Faithful geyser in yellowstone national park, WY.\n\n\n\n\n\n4.1.1.2 Sepal length of setosa irises\n# Histogram of waiting times\nhist(iris3[, \"Sepal L.\", \"Setosa\"],\n     main = \"Histogram of Sepal Length\",\n     ylab = paste0(\"Frequency (n = \", \n                   length(iris3[, \"Sepal L.\", \"Setosa\"]), \")\"),\n     xlab = \"Sepal length in cm\")\n\n# Q-Q plot of waiting times\nqqnorm(iris3[, \"Sepal L.\", \"Setosa\"],\n       main = \"Q-Q Plot of Sepal Length\")\nqqline(iris3[, \"Sepal L.\", \"Setosa\"], col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\n(a) Histogram\n\n\n\n\n\n\n\n\n\n\n\n(b) Q-Q Plot\n\n\n\n\n\n\n\nFigure 4.7: Histogram and Q-Q plot of sepal length of setosa irises in cm.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Assignment 3 - Normal distributions and the Galton board</span>"
    ]
  },
  {
    "objectID": "assignment3/rios_evr5086_hw3.html#links-to-colab-and-github",
    "href": "assignment3/rios_evr5086_hw3.html#links-to-colab-and-github",
    "title": "3  Assignment 3 - Normal distributions and the Galton board",
    "section": "4.2 Links to Colab and GitHub",
    "text": "4.2 Links to Colab and GitHub\nAssignment 2 Google Colab\nQuarto book chapter on GitHub",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Assignment 3 - Normal distributions and the Galton board</span>"
    ]
  },
  {
    "objectID": "assignment4/rios_evr5086_hw4.html",
    "href": "assignment4/rios_evr5086_hw4.html",
    "title": "4  Assignment 4",
    "section": "",
    "text": "Assignment 4\nTo complete this assignment in R, I used the following packages:\n# Check if libraries are installed; install if not.\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(here, dplyr, flextable,  ggplot2, tidyr, moments, fBasics)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Assignment 4</span>"
    ]
  },
  {
    "objectID": "assignment4/rios_evr5086_hw4.html#assignment-4",
    "href": "assignment4/rios_evr5086_hw4.html#assignment-4",
    "title": "4  Assignment 4",
    "section": "",
    "text": "here: here() enables easy file referencing\ndplyr: functions for data manipulation\ntidyr: functions for reshaping data\nflextable: formatting tables\nggplot2: for creating plots\nmoments: functions for kurtosis and skewness\nfBasics: dagoTest() for the D’Agostino normality test",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Assignment 4</span>"
    ]
  },
  {
    "objectID": "assignment4/rios_evr5086_hw4.html#exercise-1",
    "href": "assignment4/rios_evr5086_hw4.html#exercise-1",
    "title": "4  Assignment 4",
    "section": "4.1 Exercise 1",
    "text": "4.1 Exercise 1\nTo start, I read in MIA_J-D_T_Precip_inches.csv and assigned descriptive column names. Instead of computing the time series for one month at a time, I used dplyr::mutate() to divide each month’s column by the total column. This results in a data frame of the fraction of the annual total by year and month. For the remainder of the assignment, I focus on the month of April. Figure 4.1 shows the fraction of annual rainfall in April in Miami, Florida, from 1906 to 2022. Table 4.1 reports the descriptive statistics, rounded to four significant digits, for the fraction of annual rainfall occurring in the month of April.\n\n# Read in and subset MIA_J-D_T_Precip_inches.csv\nmia_rain &lt;- read.csv(here(\"assignment4\", \"data\", \"MIA_J-D_T_Precip_inches.csv\"),\n                   header = FALSE)\n\n# Add descriptive column names\nnames(mia_rain) &lt;- c(\"year\", \"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\",\n                     \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\", \"total\")\n\n# Compute monthly fraction by dividing all months by the total\nmia_rain_fraction &lt;- mia_rain %&gt;%\n      mutate(across(-c(\"year\", \"total\"), ~ round(. / total, 4)))\n\n\n#Plot the time series for April's fraction of the annual total for each year\nggplot(mia_rain_fraction, aes(x = year, y = apr)) +\n  geom_line() +\n  labs(\n    title = \"Fraction of annual rainfall during April for Miami, Florida\",\n    subtitle = \"Comparison between 1906 and 2022\",\n    x = \"Year\",\n    y = \"Fraction of annual rainfall during April\"\n  )\n\n\n\n\n\n\n\nFigure 4.1: Fraction of annual rainfall during the month of April for the City of Miami, Florida, from 1906 to 2022.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Assignment 4</span>"
    ]
  },
  {
    "objectID": "assignment4/rios_evr5086_hw4.html#exercise-2",
    "href": "assignment4/rios_evr5086_hw4.html#exercise-2",
    "title": "4  Assignment 4",
    "section": "4.2 Exercise 2",
    "text": "4.2 Exercise 2\nNext, ahead of reshaping the data, I created factor levels to be able to rearrange the data by month of the year and computed the number of unique years in the data. In preparing the data for analysis, I reformatted the data, assigned factor levels, computed the log10-transformed data. In using pipes to reshape and add variables to the data, I reduced the number of intermediate objects created. Similarly with group_by() and summarize(), I was able to calculate various summary statistics at the same time for both the fraction and the log10 fraction of Miami rainfall by month.\nThe comparison of the histograms and Q-Q plots in Figure 4.2 show that the log10 transformed data appears more normal than the original data, but still show departures from the Q-Q line.\n\n# Prep month levels for efficient ordering\nmonth_levels &lt;- c(\"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\",\n                  \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\")\n\n# Calculate n\nn_years &lt;- n_distinct(mia_rain_fraction, \"year\")\n\n# Prep data for upcoming analyses\nmia_rain_fraction_tidy &lt;- mia_rain_fraction |&gt;\n  # Remove total\n  select(-total) |&gt; \n  # Reformat to tidy format (long)\n  pivot_longer( \n    cols = !year,\n    names_to = \"month\",\n    values_to = \"fraction\"\n  ) |&gt;\n  # Log transform monthly fractions and define month factor levels\n  mutate(\n    month = factor(month, levels = month_levels, ordered = TRUE),\n    log10_fraction = log10(fraction)\n  ) |&gt;\n  # Reformat to tidy format (long)\n  pivot_longer(\n    cols = !c(year, month),\n    names_to = \"stat\",\n    values_to = \"values\"\n  ) |&gt;\n  # Sort by stat and year\n  arrange(stat, year, month) \n\n# Compute descriptive statistics for mia_rain_fraction\nmia_rain_fraction_summary &lt;- mia_rain_fraction_tidy |&gt;\n  # Run calculation on unique combinations of year and stat\n  group_by(month, stat) |&gt;\n  # Calculate summary stats\n  summarise(min = round(min(values), 4),\n            q1 = round(quantile(values, 0.25), 4), \n            median = round(median(values), 4), \n            mean = round(mean(values), 4), \n            q3 = round(quantile(values, 0.75), 4), \n            max = round(max(values), 4),\n            variance = round(var(values), 4),\n            sd = round(sd(values), 4),\n            sk = round(skewness(values), 4),\n            ku = round(kurtosis(values), 4),\n            .groups = \"drop\") |&gt;\n  # Sort data by stat and month\n  arrange(stat, month)\n\n\n# Rearrange the summary statistics to print nicely for just April\napr_summary &lt;- mia_rain_fraction_summary |&gt;\n  # Pivot the data so that the summary statistics have a long format\n  pivot_longer(-c(month, stat), names_to = \"metric\", values_to = \"value\")|&gt;\n  # Pull out the stat into respective columns\n  pivot_wider(names_from = stat, values_from = value) |&gt;\n  # Filter to just April\n  filter(month == \"apr\") |&gt;\n  # Remove the month column\n  select(-month) |&gt;\n  # Print reformatted and filtered data as a flextable\n  flextable() |&gt;\n  # Include a blank first column header\n  set_header_labels(metric = \"\") |&gt;\n  # Use simple format and autofit\n  theme_booktabs() |&gt;\n  fontsize(size = 9, part = \"all\") |&gt;\n  autofit()\n\n\n\n# Print summary table\napr_summary\n\n\n\nTable 4.1: Descriptive statistics for the fraction of annual rainfall during April for Miami, Florida between 1906 and 2022, and its log transformation.\n\n\n\nfractionlog10_fractionmin0.0010-3.0000q10.0224-1.6498median0.0470-1.3279mean0.0560-1.4140q30.0735-1.1337max0.2876-0.5412variance0.00210.1850sd0.04630.4301sk1.8372-1.0215ku5.05771.6156\n\n\n\n\n\n\n# Create histogram\nmia_rain_fraction_tidy |&gt;\n  filter(month == \"apr\", stat == \"fraction\") |&gt;\n  ggplot(aes(x = values)) +\n  geom_histogram(bins = 30) +\n  xlab(\"Fraction of annual rainfall in April\") +\n  ylab(paste0(\"Frequency (n = \", n_years, \")\"))\n\n# Create Q-Q plot\nmia_rain_fraction_tidy |&gt;\n  filter(month == \"apr\", stat == \"fraction\") |&gt;\n  ggplot(aes(sample = values)) +\n  stat_qq() + \n  stat_qq_line() +\n  xlab(\"Theoretical Quantiles\") +\n  ylab(\"Standardized residuals\")\n\n# Create histogram for log10 tranformed data\nmia_rain_fraction_tidy |&gt;\n  filter(month == \"apr\", stat == \"log10_fraction\") |&gt;\n  ggplot(aes(x = values)) +\n  geom_histogram(bins = 30) +\n  xlab(\"Log10 transformed fraction of annual rainfall in April\") +\n  ylab(paste0(\"Frequency (n = \", n_years, \")\"))\n\n# Create Q-Q plot for log10 transformed data\nmia_rain_fraction_tidy |&gt;\n  filter(month == \"apr\", stat == \"log10_fraction\") |&gt;\n  ggplot(aes(sample = values)) +\n  stat_qq() + \n  stat_qq_line() +\n  xlab(\"Theoretical Quantiles\") +\n  ylab(\"Standardized residuals\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Histogram\n\n\n\n\n\n\n\n\n\n\n\n(b) Q-Q Plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Histogram (Log10 transformed)\n\n\n\n\n\n\n\n\n\n\n\n(d) Q-Q Plot (Log10 transformed)\n\n\n\n\n\n\n\nFigure 4.2: Histogram (a) and Q-Q plot (b) for the fraction of annual rainfall during April for Miami, Florida between 1906 and 2022, and histogram (c) and Q-Q plot (d) of the corresponding log transformed data.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Assignment 4</span>"
    ]
  },
  {
    "objectID": "assignment4/rios_evr5086_hw4.html#exercise-3",
    "href": "assignment4/rios_evr5086_hw4.html#exercise-3",
    "title": "4  Assignment 4",
    "section": "4.3 Exercise 3",
    "text": "4.3 Exercise 3\nI used the descriptive statistics from the previous exercise to generate samples from a known normal distribution. I compared 1,000 versus 21 samples. Figure 4.3 shows how the Q-Q Q-Q plot improves with more samples. Meanwhile, in Figure 4.4 we can see how much worse the Q-Q plot looks with fewer samples!\n\n# Generate realizations of 1000 samples from known normal distribution\napr_rain_fraction_sample_1000 &lt;- round(\n  rnorm(n = 1000, \n        mean = mia_rain_fraction_summary |&gt;\n          filter(stat == \"fraction\",\n                 month == \"apr\") |&gt;  pull(mean),\n        sd = mia_rain_fraction_summary$sd[1]), 4\n)\n\n# Generate realization of 21 samples from known normal distribution\napr_rain_fraction_sample_21 &lt;- round(\n  rnorm(n = 21, \n        mean = mia_rain_fraction_summary |&gt;\n          filter(stat == \"fraction\",\n                 month == \"apr\") |&gt;  pull(mean),\n        sd = mia_rain_fraction_summary$sd[1]), 4\n)\n\n# Generate realization of 1000 samples from known normal log10 distribution\napr_rain_fraction_log10_sample_1000 &lt;- round(\n  rnorm(n = 1000, \n        mean = mia_rain_fraction_summary |&gt;\n          filter(stat == \"log10_fraction\",\n                 month == \"apr\") |&gt;  pull(mean),\n        sd = mia_rain_fraction_summary$sd[1]), 4\n)\n\n# Generate realization of 21 samples from known normal log10 distribution\napr_rain_fraction_log10_sample_21 &lt;- round(\n  rnorm(n = 21, \n        mean = mia_rain_fraction_summary |&gt;\n          filter(stat == \"log10_fraction\",\n                 month == \"apr\") |&gt;  pull(mean),\n        sd = mia_rain_fraction_summary$sd[1]), 4\n)\n\n# Create histogram with 1000 samples\nggplot() +\n  aes(x = apr_rain_fraction_sample_1000) +\n  geom_histogram(bins = 30) +\n  xlab(\"Realized samples of the fraction of annual rainfall in April\") +\n  ylab(\"Frequency (n = 1,000)\")\n\n# Create Q-Q plot with 1000 samples\nggplot() +\n  aes(sample = apr_rain_fraction_sample_1000) +\n  stat_qq() + \n  stat_qq_line() +\n  xlab(\"Theoretical Quantiles\") +\n  ylab(\"Standardized residuals\")\n\n# Create histogram with 1000 log10 samples\nggplot() +\n  aes(x = apr_rain_fraction_log10_sample_1000) +\n  geom_histogram(bins = 30) +\n  xlab(\"Realized samples of the log10 fraction of annual rainfall in April\") +\n  ylab(\"Frequency (n = 1000)\")\n\n# Create Q-Q plot with 1000 log10 samples\nggplot() +\n  aes(sample = apr_rain_fraction_log10_sample_1000) +\n  stat_qq() + \n  stat_qq_line() +\n  xlab(\"Theoretical Quantiles\") +\n  ylab(\"Standardized residuals\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Histogram\n\n\n\n\n\n\n\n\n\n\n\n(b) Q-Q Plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Histogram (Log10 transformed)\n\n\n\n\n\n\n\n\n\n\n\n(d) Q-Q Plot (Log10 transformed)\n\n\n\n\n\n\n\nFigure 4.3: Histogram (a) and Q-Q plot (b) for 1000 samples realized from a normal distribution based on the fraction of annual rainfall during April for Miami, Florida between 1906 and 2022, and histogram (c) and Q-Q plot (d) of the corresponding log transformed data.\n\n\n\nHere we plot samples from the known normal distribution with only 21 samples.\n# Create histogram with 21 samples\nggplot() +\n  aes(x = apr_rain_fraction_sample_21) +\n  geom_histogram(bins = 30) +\n  xlab(\"Realized samples of the fraction of annual rainfall in April\") +\n  ylab(\"Frequency (n = 21)\")\n\n# Create Q-Q plot with 21 samples\nggplot() +\n  aes(sample = apr_rain_fraction_sample_21) +\n  stat_qq() + \n  stat_qq_line() +\n  xlab(\"Theoretical Quantiles\") +\n  ylab(\"Standardized residuals\")\n\n# Create histogram with 21 log10 samples\nggplot() +\n  aes(x = apr_rain_fraction_log10_sample_21) +\n  geom_histogram(bins = 30) +\n  xlab(\"Realized samples of the log10 fraction of annual rainfall in April\") +\n  ylab(\"Frequency (n = 21)\")\n\n# Create Q-Q plot with 21 log10 samples\nggplot() +\n  aes(sample = apr_rain_fraction_log10_sample_21) +\n  stat_qq() + \n  stat_qq_line() +\n  xlab(\"Theoretical Quantiles\") +\n  ylab(\"Standardized residuals\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Histogram\n\n\n\n\n\n\n\n\n\n\n\n(b) Q-Q Plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Histogram (Log10 transformed)\n\n\n\n\n\n\n\n\n\n\n\n(d) Q-Q Plot (Log10 transformed)\n\n\n\n\n\n\n\nFigure 4.4: Histogram (a) and Q-Q plot (b) for 21 samples realized from a normal distribution based on the fraction of annual rainfall during April for Miami, Florida between 1906 and 2022, and histogram (c) and Q-Q plot (d) of the corresponding log transformed data.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Assignment 4</span>"
    ]
  },
  {
    "objectID": "assignment4/rios_evr5086_hw4.html#exercise-4",
    "href": "assignment4/rios_evr5086_hw4.html#exercise-4",
    "title": "4  Assignment 4",
    "section": "4.4 Exercise 4",
    "text": "4.4 Exercise 4\nI used dagoTest() to run the D’Agostino normality test on:\n\nthe fraction of annual rainfall for April\nthe log10 transformed fraction of annual rainfall for April\n1,000 realized samples from a normal distribution\n1,000 realized samples from a log10 transformed normal distribution\nthe waiting time between Old Faithful’s eruptions\nthe log10 transformed waiting time between Old Faithful’s eruptions\n\nIn Figure 4.5, I include histograms and Q-Q plots for the waiting time between Old Faithful’s eruptions. Table 4.2 provides key values for the normality tests listed above. In reading more about the test, I was reminded that the null hypothesis of the data being drawn from a normally distributed population is rejected when the p-value is less than the significance level (0.05). Following this decision rule, all of the tests listed above were rejected, except the samples that were realized from a normal distribution (numbers 3 and 4 above). This result aligns with the Q-Q plots shown previously, where all of them show departures from the expected theoretical quantiles, except the 1,000 realized samples that generated using a normal distribution (Figure 4.3).\n# Calculate sample size\nn_eruptions &lt;- length(faithful$waiting)\n\n# Create histogram\nfaithful |&gt;\n  ggplot(aes(x = waiting)) +\n  geom_histogram(bins = 30) +\n  xlab(\"Waiting time between Old Faithful's eruptions\") +\n  ylab(paste0(\"Frequency (n = \", n_eruptions, \")\"))\n\n# Create Q-Q plot\nfaithful |&gt;\n  ggplot(aes(sample = waiting)) +\n  stat_qq() + \n  stat_qq_line() +\n  xlab(\"Theoretical Quantiles\") +\n  ylab(\"Standardized residuals\")\n\n# Create histogram for log10 transformed data\nfaithful |&gt;\n  ggplot(aes(x = log10(waiting))) +\n  geom_histogram(bins = 30) +\n  xlab(\"Log10 transformed waiting time between Old Faithful's eruptions\") +\n  ylab(paste0(\"Frequency (n = \", n_eruptions, \")\"))\n\n# Create Q-Q plot for log10 transformed data\nfaithful |&gt;\n  ggplot(aes(sample = log10(waiting))) +\n  stat_qq() + \n  stat_qq_line() +\n  xlab(\"Theoretical Quantiles\") +\n  ylab(\"Standardized residuals\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Histogram\n\n\n\n\n\n\n\n\n\n\n\n(b) Q-Q Plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Histogram (Log10 transformed)\n\n\n\n\n\n\n\n\n\n\n\n(d) Q-Q Plot (Log10 transformed)\n\n\n\n\n\n\n\nFigure 4.5: Histogram (a) and Q-Q plot (b) for the waiting time between Old Faithful’s eruptions, and histogram (c) and Q-Q plot (d) of the corresponding log transformed data.\n\n\n\n\n\n# Normal tests on rainfall data, sample realization, and Old Faithful\ndago_mia_rain &lt;- dagoTest(mia_rain_fraction$apr)\ndago_mia_rain_log10 &lt;- dagoTest(log10(mia_rain_fraction$apr))\ndago_mia_rain_sample_1000 &lt;- dagoTest(apr_rain_fraction_sample_1000)\ndago_mia_rain_log10_sample_1000 &lt;- dagoTest(apr_rain_fraction_log10_sample_1000)\ndago_faithful &lt;- dagoTest(faithful$waiting)\ndago_faithful_log10 &lt;- dagoTest(log10(faithful$waiting))\n\n\n# Collect results in a tibble for table output\ndagoTest_results &lt;- tibble(\n  test = c(\n    \"April fraction\",\n    \"April log10(fraction)\",\n    \"April Normal sample (n = 1000)\",\n    \"April log10 Normal sample (n = 1000)\",\n    \"Old Faithful waiting\",\n    \"log10(Old Faithful waiting)\"\n  ),\n  chi_square = c(\n    round(dago_mia_rain@test$statistic[1], 4),\n    round(dago_mia_rain_log10@test$statistic[1], 4),\n    round(dago_mia_rain_sample_1000@test$statistic[1], 4),\n    round(dago_mia_rain_log10_sample_1000@test$statistic[1], 4),\n    round(dago_faithful@test$statistic[1], 4),\n    round(dago_faithful_log10@test$statistic[1], 4)\n  ),\n  p_value = c(\n    round(dago_mia_rain@test$p.value[1], 4),\n    round(dago_mia_rain_log10@test$p.value[1], 4),\n    round(dago_mia_rain_sample_1000@test$p.value[1], 4),\n    round(dago_mia_rain_log10_sample_1000@test$p.value[1], 4),\n    round(dago_faithful@test$p.value[1], 4),\n    round(dago_faithful_log10@test$p.value[1], 4)\n  )\n)\n\n\n\n# Flextable\nflextable(dagoTest_results) |&gt;\n  set_header_labels(test = \"\", chi_square = \"χ²\", p_value = \"p-value\") |&gt;\n  colformat_num(j = c(\"chi_square\", \"p_value\"), digits = 4) |&gt;\n  theme_booktabs() |&gt;\n  fontsize(size = 9, part = \"all\") |&gt;\n  autofit()\n\n\n\nTable 4.2: Chi-squared and p-value statistics from various normality tests conducted using the D’Agostino-Pearson omnibus normality test.\n\n\n\nχ²p-valueApril fraction59.32490.0000April log10(fraction)24.37910.0000April Normal sample (n = 1000)1.77360.4120April log10 Normal sample (n = 1000)1.99110.3695Old Faithful waiting109.24170.0000log10(Old Faithful waiting)55.58800.0000",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Assignment 4</span>"
    ]
  },
  {
    "objectID": "assignment4/rios_evr5086_hw4.html#exercise-5",
    "href": "assignment4/rios_evr5086_hw4.html#exercise-5",
    "title": "4  Assignment 4",
    "section": "4.5 Exercise 5",
    "text": "4.5 Exercise 5\nI also computed the ranks and return periods for April rainfall. When preparing the data for the plot, I noticed that it was necessary for the ranking to have a negative sign to generate a descending ranking. Lastly, in my first attempt of the plot with the logarithmic x-axis ticks, I realized I was accidentally doing log10 twice. When I use scale_x_log10 to adjust the x-axis scale, I noticed I need to read in the non-transformed data. That way the axis values in Figure 4.6 reflect the actual return periods in years, but the spacing is logarithmic.\n\n# Rank April rainfall and compute return period\napr_rank &lt;- mia_rain |&gt;\n  select(apr) |&gt; \n  # sort largest to smallest\n  arrange(desc(apr)) |&gt;      \n  # Create rank and compute return period\n  mutate(\n    rank = row_number(),          \n    recurrence = (n_years + 1) / rank,               \n    log10_recurrence = log10(recurrence)\n  )\n\n# Plot ranked rainfall amounts as a function of the log10 of the return period\nggplot(apr_rank, aes(x = recurrence, y = apr)) +\n  geom_point() +\n  scale_x_log10(\n    breaks = c(1, 10, 100),\n    labels = c(\"1\", \"10\", \"100\")\n  ) +\n  labs(x = \"Return period (years, log scale)\", y = \"Ranked April rainfall (inches)\")\n\n\n\n\n\n\n\nFigure 4.6: Ranked rainfall ammounts for the month of April as a function of the log10 of the retunperiod in years.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Assignment 4</span>"
    ]
  }
]